# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NRnScqn0fiN5BwgadK4LCbMaF-Tjt8rD
"""

# Commented out IPython magic to ensure Python compatibility.
#Let's import necessary dependencies 
import pandas as pd
import warnings
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
# %matplotlib inline

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score

warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', 1000)

#Read data for analysis
data=pd.read_csv('covtype.csv.zip')

print('Data Dimension:')
print('Number of Records:', data.shape[0])
print('Number of Features:', data.shape[1])

print('Feature Names')
print(data.columns)

print(data.info())

plt.figure(figsize=(6,4))
sns.countplot(y=data.dtypes ,data=data)
plt.xlabel("Data Type Count")
plt.ylabel("Data types")

data.isnull().sum()

data.describe()

print('Skewness of the below features:')
print(data.skew())

skew=data.skew()
skew_df=pd.DataFrame(skew,index=None,columns=['Skewness'])
plt.figure(figsize=(15,7))
sns.barplot(x=skew_df.index,y='Skewness',data=skew_df)
plt.xticks(rotation=90)

class_dist=data.groupby('Cover_Type').size()
class_label=pd.DataFrame(class_dist,columns=['Size'])
plt.figure(figsize=(8,6))
sns.barplot(x=class_label.index,y='Size',data=class_label)

for i,number in enumerate(class_dist):
    percent=(number/class_dist.sum())*100
    print('Cover_Type',class_dist.index[i])
    print('%.2f'% percent,'%')

data.head()

cont_data=data.loc[:,'Elevation':'Horizontal_Distance_To_Fire_Points']

binary_data=data.loc[:,'Wilderness_Area1':'Soil_Type40']

Wilderness_data=data.loc[:,'Wilderness_Area1': 'Wilderness_Area4']

Soil_data=data.loc[:,'Soil_Type1':'Soil_Type40']

for col in binary_data:
    count=binary_data[col].value_counts()
    print(col,count)

print('Soil Type',' Occurence_count')
for col in binary_data:
    count=binary_data[col].value_counts()[1] #considering all one's among 1 and 0's in each soil type
    if count < 1000:
        print(col,count)

# data_num = data.select_dtypes([np.int, np.float]) #If you need to select only numeric features. 
#Here we already have all numeric Data.

for i, col in enumerate(cont_data.columns):
    plt.figure(i)
    sns.distplot(cont_data[col])

# %%time
data['Cover_Type']=data['Cover_Type'].astype('category') #To convert target class into category

for i, col in enumerate(cont_data.columns):
    plt.figure(i,figsize=(8,4))
    sns.boxplot(x=data['Cover_Type'], y=col, data=data, palette="coolwarm")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for i, col in enumerate(binary_data.columns):
#     plt.figure(i,figsize=(6,4))
#     sns.countplot(x=col, hue=data['Cover_Type'] ,data=data, palette="rainbow")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def rev_code(row):
#     for c in Wilderness_data.columns:
#         if row[c]==1:
#             return c  
# 
# data['Wilderness_Type']=Wilderness_data.apply(rev_code, axis=1) #Time consuming

# Commented out IPython magic to ensure Python compatibility.
# %%time
# plt.figure(figsize=(16,8))
# sns.countplot(x='Wilderness_Type', hue='Cover_Type',data=data, palette="rainbow")
# plt.xticks(rotation=90)

soil_counts = []
for num in range(1,41):
    col = ('Soil_Type' + str(num))
    this_soil = data[col].groupby(data['Cover_Type'])
    totals = []
    for value in this_soil.sum():
        totals.append(value)
    total_sum = sum(totals)
    soil_counts.append(total_sum)
    print("Total Trees in Soil Type {0}: {1}".format(num, total_sum))
    percentages = [ (total*100 / total_sum) for total in totals]
    print("{0}\n".format(percentages))
print("Number of trees in each soil type:\n{0}".format(soil_counts))

plt.figure(figsize=(15,8))
sns.heatmap(cont_data.corr(),cmap='magma',linecolor='white',linewidths=1,annot=True)

g = sns.PairGrid(cont_data)
g.map(plt.scatter)

X=data.loc[:,'Elevation':'Soil_Type40']
y=data['Cover_Type']

#Features to be removed before the model
rem=['Hillshade_3pm','Soil_Type7','Soil_Type8','Soil_Type14','Soil_Type15',
     'Soil_Type21','Soil_Type25','Soil_Type28','Soil_Type36','Soil_Type37']

X.drop(rem, axis=1, inplace=True)

#Splitting the data into  train and test
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=101)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #Setup arrays to store training and test accuracies
# neighbors = np.arange(1,7)
# train_accuracy =np.empty(len(neighbors))
# test_accuracy = np.empty(len(neighbors))
# 
# for i,k in enumerate(neighbors):
#     #Setup a knn classifier with k neighbors
#     knn = KNeighborsClassifier(n_neighbors=k)
#     
#     #Fit the model
#     knn.fit(X_train, y_train)
#     
#     #Compute accuracy on the training set
#     train_accuracy[i] = knn.score(X_train, y_train)
#     
#     #Compute accuracy on the test set
#     test_accuracy[i] = knn.score(X_test, y_test)

#Generate plot
plt.figure(figsize=(10,6))
plt.title('k-NN Varying number of neighbors')
plt.plot(neighbors, test_accuracy, label='Testing Accuracy')
plt.plot(neighbors, train_accuracy, label='Training accuracy')
plt.legend()
plt.xlabel('Number of neighbors')
plt.ylabel('Accuracy')
#plt.show()

#Setup a knn classifier with k neighbors
knn = KNeighborsClassifier(n_neighbors=5) #Using Eucledian distance

#Fit the model
knn.fit(X_train,y_train)

#Get accuracy. Note: In case of classification algorithms score method represents accuracy.
Accuracy=knn.score(X_test,y_test)
print('KNN Accuracy:',Accuracy)